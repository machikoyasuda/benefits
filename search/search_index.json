{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 This website provides technical documentation for the benefits application from the California Integrated Travel Project (Cal-ITP) . Documentation for the dev (default) branch is available online at: https://docs.calitp.org/benefits . Overview \u00b6 benefits is a Django 3.x web application enabling automated eligibility verification and enrollment for transit benefits onto customers\u2019 existing contactless bank (credit/debit) cards. The application implements an Eligibility Verification API . Both the API and the application are designed for privacy and security of user information: The API communicates with signed and encrypted JSON Web Tokens containing only the most necessary of user data for the purpose of eligibility verification The application requires no user accounts and stores no information about the user Interaction with the application is anonymous, with only minimal event tracking for usage and problem analysis benefits is hosted in Amazon Web Services (AWS) Elastic Container Service (ECS) on Fargate and deployed with GitHub Actions. Running the application locally is possible with Docker and Docker Compose . The user interface and content is available in both English and Spanish. Additional language support is possible via Django\u2019s i18n and l10n features .","title":"Home"},{"location":"#home","text":"This website provides technical documentation for the benefits application from the California Integrated Travel Project (Cal-ITP) . Documentation for the dev (default) branch is available online at: https://docs.calitp.org/benefits .","title":"Home"},{"location":"#overview","text":"benefits is a Django 3.x web application enabling automated eligibility verification and enrollment for transit benefits onto customers\u2019 existing contactless bank (credit/debit) cards. The application implements an Eligibility Verification API . Both the API and the application are designed for privacy and security of user information: The API communicates with signed and encrypted JSON Web Tokens containing only the most necessary of user data for the purpose of eligibility verification The application requires no user accounts and stores no information about the user Interaction with the application is anonymous, with only minimal event tracking for usage and problem analysis benefits is hosted in Amazon Web Services (AWS) Elastic Container Service (ECS) on Fargate and deployed with GitHub Actions. Running the application locally is possible with Docker and Docker Compose . The user interface and content is available in both English and Spanish. Additional language support is possible via Django\u2019s i18n and l10n features .","title":"Overview"},{"location":"deployment/","text":"Overview \u00b6 The application is deployed to AWS Elastic Container Service (ECS) using a Task Definition generated from the template file at .aws/ecs-task.json . The application is deployed into three separate AWS environments for dev , test , and prod . GitHub Environments exist corresponding to each AWS deployment environment, with secrets and protection rules specific to each GitHub Environment. A GitHub Action per Environment is responsible for deploying that Environment\u2019s branch to the corresponding AWS location. Configuration \u00b6 Configuration data (based on .devcontainer/.env.sample and fixtures/ ) is stored in AWS S3 buckets for each deployment environment. ECS runtime \u00b6 The ECS Task Definition includes a containerDefinition , using the AWS CLI Docker image, to pull the fixture data from the corresponding S3 bucket during service (re)start. This configuration is copied into a volume that is also mounted into the main application container. The main application containerDefinition uses dependsOn to ensure that the AWS CLI container task has completed successfully, before starting itself. Both containers use the environmentFiles setting to load an .env file from their deploy environment\u2019s S3 bucket. Local AWS \u00b6 Warning The following command will decrypt and download the benefits configuration from S3 into the .aws/config directory on your local computer. Be sure this is what you want to do. To copy the AWS configuration locally, fill in the appropriate values in your local .env file: for the AWS connection: AWS_DEFAULT_REGION=us-west-2 AWS_ACCESS_KEY_ID=access-key-id AWS_SECRET_ACCESS_KEY=secret-access-key AWS_BUCKET=bucket-name and to ensure Django uses the downloaded configuration: DJANGO_INIT_PATH=config/<file>.json and then pull the files down to your local computer: docker compose run s3pull Update AWS \u00b6 Warning The following command will send the entire contents of the .aws/config directory from your local computer into the benefits S3 bucket for the configured environment. Be sure this is what you want to do. A Docker Compose service can also be used to push updates to the configuration data into S3 for the given deploy environment: Ensure you have content (e.g. an .env or config.json file) inside .aws/config in your local repository and then run: docker compose run s3push","title":"Overview"},{"location":"deployment/#overview","text":"The application is deployed to AWS Elastic Container Service (ECS) using a Task Definition generated from the template file at .aws/ecs-task.json . The application is deployed into three separate AWS environments for dev , test , and prod . GitHub Environments exist corresponding to each AWS deployment environment, with secrets and protection rules specific to each GitHub Environment. A GitHub Action per Environment is responsible for deploying that Environment\u2019s branch to the corresponding AWS location.","title":"Overview"},{"location":"deployment/#configuration","text":"Configuration data (based on .devcontainer/.env.sample and fixtures/ ) is stored in AWS S3 buckets for each deployment environment.","title":"Configuration"},{"location":"deployment/#ecs-runtime","text":"The ECS Task Definition includes a containerDefinition , using the AWS CLI Docker image, to pull the fixture data from the corresponding S3 bucket during service (re)start. This configuration is copied into a volume that is also mounted into the main application container. The main application containerDefinition uses dependsOn to ensure that the AWS CLI container task has completed successfully, before starting itself. Both containers use the environmentFiles setting to load an .env file from their deploy environment\u2019s S3 bucket.","title":"ECS runtime"},{"location":"deployment/#local-aws","text":"Warning The following command will decrypt and download the benefits configuration from S3 into the .aws/config directory on your local computer. Be sure this is what you want to do. To copy the AWS configuration locally, fill in the appropriate values in your local .env file: for the AWS connection: AWS_DEFAULT_REGION=us-west-2 AWS_ACCESS_KEY_ID=access-key-id AWS_SECRET_ACCESS_KEY=secret-access-key AWS_BUCKET=bucket-name and to ensure Django uses the downloaded configuration: DJANGO_INIT_PATH=config/<file>.json and then pull the files down to your local computer: docker compose run s3pull","title":"Local AWS"},{"location":"deployment/#update-aws","text":"Warning The following command will send the entire contents of the .aws/config directory from your local computer into the benefits S3 bucket for the configured environment. Be sure this is what you want to do. A Docker Compose service can also be used to push updates to the configuration data into S3 for the given deploy environment: Ensure you have content (e.g. an .env or config.json file) inside .aws/config in your local repository and then run: docker compose run s3push","title":"Update AWS"},{"location":"deployment/workflows/","text":"Workflows \u00b6 There are three different GitHub Actions deployment workflows, one for each environment: .github/workflows/deploy-dev.yml .github/workflows/deploy-test.yml .github/workflows/deploy-prod.yml Info The entire process from GitHub commit to full redeploy of the application can take from around 5 minutes to 10 minutes or more depending on the deploy environment. Have patience! Deployment steps \u00b6 Each of the three workflows are triggered with a push to the corresponding branch. Each workflow also responds to the workflow_dispatch event to allow manually triggering via the GitHub Actions UI. When a deployment workflow runs, the following steps are taken: 1. Checkout code \u00b6 From the tip of the corresponding branch (e.g. dev ) 2. Authenticate to AWS \u00b6 Using secrets defined in the corresponding GitHub environment (e.g. dev ) 3. Build and push image to ECR \u00b6 Build the root Dockerfile , tagging with the SHA from the checked-out commit. Push this main application image/tag into an ECR corresponding to the deploy environment in AWS. Using the same ECR information, the (static) path to the configuration image is also output for use later in the workflow. 4. Generate ECS Task Definition \u00b6 The .aws/ecs-task.json file serves as a template from which the corresponding ECS Task Definition is generated, with build and environment-specific information filled in. Values wrapped in angle brackets, such as <aws_account> and <aws_bucket> , are replaced in the template by their corresponding secret from the GitHub environment. The image names/tags generated from the ECR push step are inserted into the container definitions. 5. Deploy Task Definition to ECS \u00b6 The final step is deploying the newly created Task Definition to the Amazon ECS cluster. Once deployed, ECS does the following: Drains existing connections Increments service version number Restarts the service The GitHub Actions workflows wait for the service to restart and to reach a steady state before marking successful completion. How do the workflows differ? \u00b6 The only differences between the three workflow files are the name : -name: Deploy to Amazon ECS (dev) +name: Deploy to Amazon ECS (test) the branch filter: on: push: branches: - - dev + - test and the environment (and related concurrency setting) to load with each run: jobs: deploy: runs-on: ubuntu-latest - environment: dev - concurrency: dev + environment: test + concurrency: test If in the future GitHub allows for templated workflows or branch-matrix support, these definitions may be collapsed into a single file.","title":"Workflows"},{"location":"deployment/workflows/#workflows","text":"There are three different GitHub Actions deployment workflows, one for each environment: .github/workflows/deploy-dev.yml .github/workflows/deploy-test.yml .github/workflows/deploy-prod.yml Info The entire process from GitHub commit to full redeploy of the application can take from around 5 minutes to 10 minutes or more depending on the deploy environment. Have patience!","title":"Workflows"},{"location":"deployment/workflows/#deployment-steps","text":"Each of the three workflows are triggered with a push to the corresponding branch. Each workflow also responds to the workflow_dispatch event to allow manually triggering via the GitHub Actions UI. When a deployment workflow runs, the following steps are taken:","title":"Deployment steps"},{"location":"deployment/workflows/#1-checkout-code","text":"From the tip of the corresponding branch (e.g. dev )","title":"1. Checkout code"},{"location":"deployment/workflows/#2-authenticate-to-aws","text":"Using secrets defined in the corresponding GitHub environment (e.g. dev )","title":"2. Authenticate to AWS"},{"location":"deployment/workflows/#3-build-and-push-image-to-ecr","text":"Build the root Dockerfile , tagging with the SHA from the checked-out commit. Push this main application image/tag into an ECR corresponding to the deploy environment in AWS. Using the same ECR information, the (static) path to the configuration image is also output for use later in the workflow.","title":"3. Build and push image to ECR"},{"location":"deployment/workflows/#4-generate-ecs-task-definition","text":"The .aws/ecs-task.json file serves as a template from which the corresponding ECS Task Definition is generated, with build and environment-specific information filled in. Values wrapped in angle brackets, such as <aws_account> and <aws_bucket> , are replaced in the template by their corresponding secret from the GitHub environment. The image names/tags generated from the ECR push step are inserted into the container definitions.","title":"4. Generate ECS Task Definition"},{"location":"deployment/workflows/#5-deploy-task-definition-to-ecs","text":"The final step is deploying the newly created Task Definition to the Amazon ECS cluster. Once deployed, ECS does the following: Drains existing connections Increments service version number Restarts the service The GitHub Actions workflows wait for the service to restart and to reach a steady state before marking successful completion.","title":"5. Deploy Task Definition to ECS"},{"location":"deployment/workflows/#how-do-the-workflows-differ","text":"The only differences between the three workflow files are the name : -name: Deploy to Amazon ECS (dev) +name: Deploy to Amazon ECS (test) the branch filter: on: push: branches: - - dev + - test and the environment (and related concurrency setting) to load with each run: jobs: deploy: runs-on: ubuntu-latest - environment: dev - concurrency: dev + environment: test + concurrency: test If in the future GitHub allows for templated workflows or branch-matrix support, these definitions may be collapsed into a single file.","title":"How do the workflows differ?"},{"location":"getting-started/","text":"Local setup \u00b6 Running the Benefits application in a local, non-production environment requires Docker . The following commands should be run in a terminal program like bash . Clone the repository \u00b6 git clone https://github.com/cal-itp/benefits Change into the .devcontainer dir \u00b6 This is where configuration for running locally is stored. cd benefits/.devcontainer Create an environment file \u00b6 Use the sample as a template, the default values will work for now. cp .env.sample .env Build image using Docker Compose \u00b6 docker compose build --no-cache client Start the client \u00b6 docker compose up [ -d ] client The optional -d flag will start in detatched mode and allow you to continue using the terminal session. Otherwise your terminal will be attached to the container\u2019s terminal, showing the startup and runtime output. After initialization, the client is running running on http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information on accessing the site on localhost. If DJANGO_ADMIN=true , the backend administrative interface can be accessed at the /admin route using the superuser account you setup as part of initialization. By default, sample data is used to initialize Django. Alternatively you may: Modify the sample data file(s); or Point DJANGO_INIT_PATH at different data file(s); or Use production data stored in S3 (see Deployment ); or (If DJANGO_ADMIN=true ) use the backend administrative interface CRUD Stop the running services with: docker compose down","title":"Local setup"},{"location":"getting-started/#local-setup","text":"Running the Benefits application in a local, non-production environment requires Docker . The following commands should be run in a terminal program like bash .","title":"Local setup"},{"location":"getting-started/#clone-the-repository","text":"git clone https://github.com/cal-itp/benefits","title":"Clone the repository"},{"location":"getting-started/#change-into-the-devcontainer-dir","text":"This is where configuration for running locally is stored. cd benefits/.devcontainer","title":"Change into the .devcontainer dir"},{"location":"getting-started/#create-an-environment-file","text":"Use the sample as a template, the default values will work for now. cp .env.sample .env","title":"Create an environment file"},{"location":"getting-started/#build-image-using-docker-compose","text":"docker compose build --no-cache client","title":"Build image using Docker Compose"},{"location":"getting-started/#start-the-client","text":"docker compose up [ -d ] client The optional -d flag will start in detatched mode and allow you to continue using the terminal session. Otherwise your terminal will be attached to the container\u2019s terminal, showing the startup and runtime output. After initialization, the client is running running on http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information on accessing the site on localhost. If DJANGO_ADMIN=true , the backend administrative interface can be accessed at the /admin route using the superuser account you setup as part of initialization. By default, sample data is used to initialize Django. Alternatively you may: Modify the sample data file(s); or Point DJANGO_INIT_PATH at different data file(s); or Use production data stored in S3 (see Deployment ); or (If DJANGO_ADMIN=true ) use the backend administrative interface CRUD Stop the running services with: docker compose down","title":"Start the client"},{"location":"getting-started/development/","text":"Development \u00b6 Branches and merging \u00b6 The default GitHub branch is dev . All new feature work should be in the form of Pull Requests (PR) that target dev as their base. In addition to dev , the repository has three other long-lived branches: test and prod correspond to the Test and Production deploy environments , respectively. gh-pages hosts the compiled documentation, and is always forced-pushed by the docs build process . Branch protection rules are in place on three environment branches ( dev , test , prod ) to: Prevent branch deletion Restrict force-pushing, where appropriate Require passing status checks before merging into the target branch is allowed Merging of PRs should be done using the merge commit strategy. The squash strategy is also enabled for particularly wild or messy PRs, but this should only be used as a last resort when it is not possible or too difficult to rebase the PR branch onto the target branch before merging. When merging a PR into dev , it is customary to format the merge commit message like: Title of PR (#number) instead of the default: Merge pull request #number from source-repo/source-branch Application deployments occur automatically when a PR is merged to the target environment branch. A successful deploy to dev is required before a deploy to test is allowed; a successful deploy to test is required before a deploy to prod is allowed. See Deployment for more information. pre-commit \u00b6 This repository uses pre-commit hooks to check and format code. Branch protection rules on the environment branches in GitHub ensure that pre-commit checks have passed before a merge is allowed. VS Code with Devcontainers \u00b6 Tip VS Code with Devcontainers is the recommended development setup Warning You must build the base Docker image benefits_client:latest before running the devcontainer. See Local Setup VS Code can be used together with Docker via the Remote - Containers extension to enable a container-based development environment. This repository includes a .devcontainer.json file that configures remote container development and debugging. With the Remote - Containers extension enabled, open the folder containing this repository inside Visual Studio Code. You should receive a prompt in the Visual Studio Code window; click Reopen in Container to run the development environment inside a container. If you do not receive a prompt, or when you feel like starting from a fresh environment: Ctrl/Cmd+Shift+P to bring up the command palette in Visual Studio Code Type Remote-Containers to filter the commands Select Rebuild and Reopen in Container Attach a debugger \u00b6 Once running inside a container, press F5 to attach a debugger to the client, running on http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information. Add breakpoints in the code and browse the local site to trigger a pause. Press F5 to continue execution from the breakpoint. Exiting devcontainers \u00b6 To close out of the container and re-open the directory locally in Visual Studio Code: Ctrl/Cmd+Shift+P to bring up the command palette in Visual Studio Code Type Remote-Containers to filter the commands Select Reopen Locally Test Eligibility Verification server \u00b6 A basic eligibility verification server is available for testing. The server code is available on GitHub , with its own set of documentation . Running locally \u00b6 docker compose up [ -d ] server The optional -d flag will start in detatched mode and allow you to continue using the terminal session. Otherwise your terminal will be attached to the container\u2019s terminal, showing the startup and runtime output. The API server is running on http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information on accessing the server on localhost. From within another Compose service container, the server is at http://server:5000 using the service-forwarding features of Compose. In either case, the endpoint /verify serves as the Eligibility Verification API endpoint. In the Devcontainer \u00b6 When running the Devcontainer , the server is automatically started. See Docker dynamic ports for more information on accessing the server on localhost. The server is accessible from within the Devcontainer at its Compose service address: http://server:5000 .","title":"Development"},{"location":"getting-started/development/#development","text":"","title":"Development"},{"location":"getting-started/development/#branches-and-merging","text":"The default GitHub branch is dev . All new feature work should be in the form of Pull Requests (PR) that target dev as their base. In addition to dev , the repository has three other long-lived branches: test and prod correspond to the Test and Production deploy environments , respectively. gh-pages hosts the compiled documentation, and is always forced-pushed by the docs build process . Branch protection rules are in place on three environment branches ( dev , test , prod ) to: Prevent branch deletion Restrict force-pushing, where appropriate Require passing status checks before merging into the target branch is allowed Merging of PRs should be done using the merge commit strategy. The squash strategy is also enabled for particularly wild or messy PRs, but this should only be used as a last resort when it is not possible or too difficult to rebase the PR branch onto the target branch before merging. When merging a PR into dev , it is customary to format the merge commit message like: Title of PR (#number) instead of the default: Merge pull request #number from source-repo/source-branch Application deployments occur automatically when a PR is merged to the target environment branch. A successful deploy to dev is required before a deploy to test is allowed; a successful deploy to test is required before a deploy to prod is allowed. See Deployment for more information.","title":"Branches and merging"},{"location":"getting-started/development/#pre-commit","text":"This repository uses pre-commit hooks to check and format code. Branch protection rules on the environment branches in GitHub ensure that pre-commit checks have passed before a merge is allowed.","title":"pre-commit"},{"location":"getting-started/development/#vs-code-with-devcontainers","text":"Tip VS Code with Devcontainers is the recommended development setup Warning You must build the base Docker image benefits_client:latest before running the devcontainer. See Local Setup VS Code can be used together with Docker via the Remote - Containers extension to enable a container-based development environment. This repository includes a .devcontainer.json file that configures remote container development and debugging. With the Remote - Containers extension enabled, open the folder containing this repository inside Visual Studio Code. You should receive a prompt in the Visual Studio Code window; click Reopen in Container to run the development environment inside a container. If you do not receive a prompt, or when you feel like starting from a fresh environment: Ctrl/Cmd+Shift+P to bring up the command palette in Visual Studio Code Type Remote-Containers to filter the commands Select Rebuild and Reopen in Container","title":"VS Code with Devcontainers"},{"location":"getting-started/development/#attach-a-debugger","text":"Once running inside a container, press F5 to attach a debugger to the client, running on http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information. Add breakpoints in the code and browse the local site to trigger a pause. Press F5 to continue execution from the breakpoint.","title":"Attach a debugger"},{"location":"getting-started/development/#exiting-devcontainers","text":"To close out of the container and re-open the directory locally in Visual Studio Code: Ctrl/Cmd+Shift+P to bring up the command palette in Visual Studio Code Type Remote-Containers to filter the commands Select Reopen Locally","title":"Exiting devcontainers"},{"location":"getting-started/development/#test-eligibility-verification-server","text":"A basic eligibility verification server is available for testing. The server code is available on GitHub , with its own set of documentation .","title":"Test Eligibility Verification server"},{"location":"getting-started/development/#running-locally","text":"docker compose up [ -d ] server The optional -d flag will start in detatched mode and allow you to continue using the terminal session. Otherwise your terminal will be attached to the container\u2019s terminal, showing the startup and runtime output. The API server is running on http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information on accessing the server on localhost. From within another Compose service container, the server is at http://server:5000 using the service-forwarding features of Compose. In either case, the endpoint /verify serves as the Eligibility Verification API endpoint.","title":"Running locally"},{"location":"getting-started/development/#in-the-devcontainer","text":"When running the Devcontainer , the server is automatically started. See Docker dynamic ports for more information on accessing the server on localhost. The server is accessible from within the Devcontainer at its Compose service address: http://server:5000 .","title":"In the Devcontainer"},{"location":"getting-started/docker-dynamic-ports/","text":"Docker dynamic ports \u00b6 Docker dynamically assigns host machine ports that map into container application ports. Inside the Devcontainer \u00b6 Info The Devcontainer can bind to a single container\u2019s port(s) and present those to your localhost machine via VS Code. Other services started along with the Devcontainer are not visible in VS Code. See Outside the Devconatiner for how to find information on those. Once started with F5 , the benefits Django application runs on port 8000 inside the Devcontainer. To find the localhost address, look on the PORTS tab in VS Code\u2019s Terminal window. The Local Address corresponding to the record where 8000 is in the Port column is where the site is accessible on your host machine. Replace 0.0.0.0 with localhost and use the same port number shown in the Local Address column. This is highlighted by the red box in the image below: Outside the Devcontainer \u00b6 When running a docker compose ... command, or in other scenarios outside of the Devcontainer, there are multiple ways to find the http://localhost port corresponding to the service in question. Docker Desktop \u00b6 The Docker Desktop application shows information about running containers and services/groups, including information about bound ports. In most cases, the application provides a button to launch a container/service directly in your browser when a port binding is available. In the Containers / Apps tab, expand the service group if needed to find the container in question, where you should see labels indicating the container is RUNNING and bound to PORT: XYZ . Hover over the container in question, and click the Open in Browser button to launch the app in your web browser. Docker CLI commands \u00b6 Using the docker command line interface, you can find the bound port(s) of running containers. docker ps -f name = <service> e.g. for the docs service: docker ps -f name = docs This prints output like the following: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0d5b2e1fb910 benefits_client:dev \"mkdocs serve --dev-\u2026\" 2 minutes ago Up 2 minutes 0.0.0.0:62093->8000/tcp benefits_docs_1 Looking at the PORTS column: PORTS 0.0.0.0:62093->8000/tcp We can see that locally, port 62093 is bound to the container port 8000 . In this case, entering http://localhost:62093 in the web browser navigates to the docs site homepage.","title":"Docker dynamic ports"},{"location":"getting-started/docker-dynamic-ports/#docker-dynamic-ports","text":"Docker dynamically assigns host machine ports that map into container application ports.","title":"Docker dynamic ports"},{"location":"getting-started/docker-dynamic-ports/#inside-the-devcontainer","text":"Info The Devcontainer can bind to a single container\u2019s port(s) and present those to your localhost machine via VS Code. Other services started along with the Devcontainer are not visible in VS Code. See Outside the Devconatiner for how to find information on those. Once started with F5 , the benefits Django application runs on port 8000 inside the Devcontainer. To find the localhost address, look on the PORTS tab in VS Code\u2019s Terminal window. The Local Address corresponding to the record where 8000 is in the Port column is where the site is accessible on your host machine. Replace 0.0.0.0 with localhost and use the same port number shown in the Local Address column. This is highlighted by the red box in the image below:","title":"Inside the Devcontainer"},{"location":"getting-started/docker-dynamic-ports/#outside-the-devcontainer","text":"When running a docker compose ... command, or in other scenarios outside of the Devcontainer, there are multiple ways to find the http://localhost port corresponding to the service in question.","title":"Outside the Devcontainer"},{"location":"getting-started/docker-dynamic-ports/#docker-desktop","text":"The Docker Desktop application shows information about running containers and services/groups, including information about bound ports. In most cases, the application provides a button to launch a container/service directly in your browser when a port binding is available. In the Containers / Apps tab, expand the service group if needed to find the container in question, where you should see labels indicating the container is RUNNING and bound to PORT: XYZ . Hover over the container in question, and click the Open in Browser button to launch the app in your web browser.","title":"Docker Desktop"},{"location":"getting-started/docker-dynamic-ports/#docker-cli-commands","text":"Using the docker command line interface, you can find the bound port(s) of running containers. docker ps -f name = <service> e.g. for the docs service: docker ps -f name = docs This prints output like the following: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0d5b2e1fb910 benefits_client:dev \"mkdocs serve --dev-\u2026\" 2 minutes ago Up 2 minutes 0.0.0.0:62093->8000/tcp benefits_docs_1 Looking at the PORTS column: PORTS 0.0.0.0:62093->8000/tcp We can see that locally, port 62093 is bound to the container port 8000 . In this case, entering http://localhost:62093 in the web browser navigates to the docs site homepage.","title":"Docker CLI commands"},{"location":"getting-started/documentation/","text":"Documentation \u00b6 This website is built using mkdocs from the contents of the dev (default) branch. The mkdocs.yml file in the repository root configures the build process, including the available plugins. Editing \u00b6 All content lives under the docs/ directory in the repository. To add new sections/articles, create new directories and files under the docs/ directory, in Markdown format. The pencil icon is a shortcut to quickly edit the content of the page you are viewing on the website: Above: Screenshot showing the edit pencil circled in red Features \u00b6 Material for MkDocs: Reference See mkdocs.yml for enabled plugins/features Mermaid Use code fences with mermaid type to render Mermaid diagrams within docs. For example, this markdown: ```mermaid graph LR Start --> Stop ``` Yields this diagram: graph LR Start --> Stop Running locally \u00b6 The documentation website can be run locally using Docker Compose: # from inside the .devcontainer/ directory docker compose up docs The site is served from http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information. The website is automatically rebuilt as changes are made to docs/ files. In the Devcontainer \u00b6 When running the Devcontainer , the docs site is automatically started. See Docker dynamic ports for more information on accessing the site on localhost. Deploying \u00b6 A GitHub Action watches for pushes to dev , and uses mhausenblas/mkdocs-deploy-gh-pages to build the mkdocs content, force-pushing to the gh-pages branch. At that point, GitHub Pages redeploys the docs site.","title":"Documentation"},{"location":"getting-started/documentation/#documentation","text":"This website is built using mkdocs from the contents of the dev (default) branch. The mkdocs.yml file in the repository root configures the build process, including the available plugins.","title":"Documentation"},{"location":"getting-started/documentation/#editing","text":"All content lives under the docs/ directory in the repository. To add new sections/articles, create new directories and files under the docs/ directory, in Markdown format. The pencil icon is a shortcut to quickly edit the content of the page you are viewing on the website: Above: Screenshot showing the edit pencil circled in red","title":"Editing"},{"location":"getting-started/documentation/#features","text":"Material for MkDocs: Reference See mkdocs.yml for enabled plugins/features Mermaid Use code fences with mermaid type to render Mermaid diagrams within docs. For example, this markdown: ```mermaid graph LR Start --> Stop ``` Yields this diagram: graph LR Start --> Stop","title":"Features"},{"location":"getting-started/documentation/#running-locally","text":"The documentation website can be run locally using Docker Compose: # from inside the .devcontainer/ directory docker compose up docs The site is served from http://localhost at a port dynamically assigned by Docker. See Docker dynamic ports for more information. The website is automatically rebuilt as changes are made to docs/ files.","title":"Running locally"},{"location":"getting-started/documentation/#in-the-devcontainer","text":"When running the Devcontainer , the docs site is automatically started. See Docker dynamic ports for more information on accessing the site on localhost.","title":"In the Devcontainer"},{"location":"getting-started/documentation/#deploying","text":"A GitHub Action watches for pushes to dev , and uses mhausenblas/mkdocs-deploy-gh-pages to build the mkdocs content, force-pushing to the gh-pages branch. At that point, GitHub Pages redeploys the docs site.","title":"Deploying"},{"location":"getting-started/testing/","text":"Automated tests \u00b6 Integration \u00b6 End-to-end integration tests are implemented with cypress and can be found in the tests/e2e directory in the repository. See the cypress Command Line guide for more information. Running in the Dev Container \u00b6 cypress is installed and available to run directly in the devcontainer. Ensure your .env file has an updated CYPRESS_baseUrl variable: # using localhost since we're inside the container CYPRESS_baseURL=http://localhost:8000 Rebuild and Reopen the devcontainer Start the benefits app with F5 From within the tests/e2e directory: npx cypress run","title":"Automated tests"},{"location":"getting-started/testing/#automated-tests","text":"","title":"Automated tests"},{"location":"getting-started/testing/#integration","text":"End-to-end integration tests are implemented with cypress and can be found in the tests/e2e directory in the repository. See the cypress Command Line guide for more information.","title":"Integration"},{"location":"getting-started/testing/#running-in-the-dev-container","text":"cypress is installed and available to run directly in the devcontainer. Ensure your .env file has an updated CYPRESS_baseUrl variable: # using localhost since we're inside the container CYPRESS_baseURL=http://localhost:8000 Rebuild and Reopen the devcontainer Start the benefits app with F5 From within the tests/e2e directory: npx cypress run","title":"Running in the Dev Container"}]}